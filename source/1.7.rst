千辛万苦训练好模型, 上线遇到问题? Paddle预测库轻松搞定模型部署
===============


深度学习一般分为训练和推理两个部分, 训练是神经网络"学习"的过程, 主要关注如何搜索和求解模型参数, 发现训练数据中的规律. 有了训练好的模型之后, 就要在在线环境中使用模型, 这个过程叫做推理, 推理过程中主要关注计算的性能. 飞桨提供了性能强劲, 上手简单的推理引擎来帮助用户更好地部署深度学习服务, 让用户专注于数据处理, 而不用担心底层的优化问题.


经过多个版本的升级迭代, 飞桨推理引擎针对不同环境提供了多种性能优化方法, 让用户得到最优的性能体验. 同时, 为了满足不同用户的需求, 还提供了多种常用语言的调用接口.


.. note::

    Paddle预测推荐使用C++和Python API,这两种语言API经过了严格的测试和多个版本的迭代,稳定可靠.

    .. tip::


        `Paddle预测已支持语言`

        ==========  ======  ======  ======  ======  ======
        language    C++     Python  C       Go      R 
        ==========  ======  ======  ======  ======  ======
        API         完整    完整    完整    完整    完整
        性能        对齐    对齐    对齐    对齐    待对齐
        ==========  ======  ======  ======  ======  ======



下面我们一起来看看如何使用飞桨推理引擎吧!


1. 模型保存
------------

首先, 我们需要训练并保存一个模型, Paddle提供了一个内置的函数 `save_inference_model`, 将训练模型保存为预测模型.

.. code:: python
    
    from paddle import fluid

    place = fluid.CPUPlace()
    executor = fluid.Executor(place)

    image = fluid.data(name="image", shape=[None, 28, 28], dtype="float32")
    label = fluid.data(name="label", shape=[None, 1], dtype="int64")

    feeder = fluid.DataFeeder(feed_list=[image, label], place=place)
    predict = fluid.layers.fc(input=image, size=10, act='softmax')

    loss = fluid.layers.cross_entropy(input=predict, label=label)
    avg_loss = fluid.layers.mean(loss)

    executor.run(fluid.default_startup_program())

    # 保存预测模型到model目录中, 只保存与输入image和输出predict相关的部分网络
    fluid.io.save_inference_model("model", feed_var_names=["image"],
        target_vars=[predict]. executor=executor)


.. tip::

    `save_inference_model`根据预测需要的输入和输出, 对训练模型进行剪枝, 去除和预测无关部分, 得到的模型相比训练更加精简, 适合优化和部署.


2. 预测加载
-----------

有了预测模型之后, 就可以使用预测库了, Paddle提供了 AnalysisConfig 用于管理预测部署的各种设置, 用户可以根据自己的上线环境, 打开各种优化.

首先我们创建一个config

.. code:: python

    from paddle.fluid.core import AnalysisConfig

    # 创建config
    config = AnalysisConfig("./model")



在Intel CPU上, 若硬件支持, 可以打开 `DNNL`_ (Deep Neural Network Library, 原名MKLDNN) 优化, 这是一个Intel开源的高性能计算库, 用于Intel架构的处理器和图形处理器上的深度学习优化, 飞桨推理引擎在后端将自动调用.

.. _DNNL: https://github.com/intel/mkl-dnn.git


.. code:: python

    config.enable_mkldnn()



对于需要使用Nvidia GPU用户, 只需要一行配置, 飞桨就会自动将计算切换到GPU上

.. code:: python

    # 在GPU 0上初始化100M显存, 这只是一个初始值, 实际显存可能会动态变化
    config.enable_use_gpu(100, 0)


飞桨推理引擎提供了zero copy的方式管理输入和输出, 减少拷贝

.. code:: python

    # 打开zero copy
    config.switch_use_feed_fetch_ops(False)
    config.switch_specify_input_names(True)


设置好预测的配置之后, 就可以创建predictor了


.. code:: python

    from paddle.fluid.core import create_paddle_predictor

    predictor = create_paddle_predictor(config)


.. tip::

    Paddle预测提供了多项图优化, 创建predictor时将会加载预测模型并自动运行图优化, 以提高预测性能.


3. 运行
------------

创建好predictor之后, 只需要传入数据就可以运行预测了, 这里假设我们已经将输入数据读入了一个numpy.ndarray数组中.


Paddle 提供了简单易用的API来管理输入和输出. 首先将输入数据传入predictor


.. code:: python

    input_names = predictor.get_input_names()
    # 得到输入ZeroCopyTensor, 前面保存的模型只有一个输入image, 若有多个输入类似
    input_tensor = predictor.get_input_tensor(input_names[0])

    input_tensor.copy_from_cpu(input_data.reshape([1, 28, 28]).astype("float32"))


运行推理引擎, 这里将会执行真正的计算


.. code:: python

    predictor.zero_copy_run()


解析结果到一个numpy数组中


.. code:: python

    ouput_names = predictor.get_output_names()
    # 获取输出ZeroCopyTensor
    output_tensor = predictor.get_output_tensor(output_names[0])

    # 得到一个numpy.ndarray结构的输出数据
    output_data = output_tensor.copy_to_cpu()



4. 进阶
-------------

4.1 使用TensorRT加速预测
~~~~~~~~~~~~

Paddle预测集成了TensorRT引擎, 使用GPU时, 打开TensorRT在一些模型上可以提高性能


.. code:: python

    config.enable_tensorrt_engine(precision_mode=AnalysisConfig.Precision.Float32,
                                  use_calib_mode=True)


4.2 使用Paddle-Lite优化
~~~~~~~~
Paddle-Lite


4.3 在其他语言中使用Paddle预测
~~~~~~~

        
